---
title: "Customer Analysis and Promotion Response Prediction"
author: "Joshua Harasaki, Tiffany Chiang, Katie Huynh"
date: "3/15/2022"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(ggExtra)
library(cluster)
library(ggridges)
library(factoextra)
library(scales)
library(caret)
library(glmnet)
library(ISLR)
library(dplyr)
library(rsample)
library(gbm)
library(ggstatsplot)
library(lares)
```

# Introduction

Our goal for this project is to use machine learning models on a set of marketing data to assist with customer segmentation and prediction of a customer's response to a firm's marketing campaigns.

### What is customer segmentation?

According to [Qualtrics](https://www.qualtrics.com/experience-management/brand/customer-segmentation/), a leading customer experience management firm, "Customer segmentation is an effective tool for businesses to closely align their strategy and tactics, with and better target, their customers."

Customer segmentation is the practice of separating customers into groups based on common characteristics, such as demographics or behaviors, so companies can market to those customers more effectively. In addition, segmenting customers allows you to get a better understanding of your customer's needs and desires, which increases a company's customer lifetime value. Customer segmentation groups can be used to begin discussions of building a marketing persona, and can shape a brand's messaging. For example, a company can analyze which customer segment is most likely to purchase a certain product and use this information to create tailored marketing strategies.

#### Customer segments can be profiled based on different attributes:

1)  Segmenting customers based on *who they are*:

-   Age
-   Geography
-   Income
-   Relationship Status
-   Family
-   Job Type
-   etc.

2)  Segmenting customers based on *what they do*:

-   Basket size - define this??
-   Average spend
-   Tenure (How long customer stays with you)
-   Product purchase history
-   Time elapsed since last purchase
-   etc.

Customer segments are not limited to just one attribute; they can be based on a combination of different variables.

### Project Objectives

1\. Group our customers based on demographic, lifestyle and behavioral data

2\. Create classification models to predict which customers will respond to marketing campaigns

## Overview of Data Set

Let's start by exploring the data set on hand. We begin with loading data and packages.

```{r}
#Loading the data
data = read.csv('data/unprocessed/marketing_campaign.csv', sep = '\t')
head(data)
```

This data is collected from a marketing campaign from an unspecified company.

Our data set includes 2240 observations and 29 variables. The variables can be classified in 4 categories: People, Products, Promotion, and Place. Respectively, these describe customer demographic, amount spent on each type of product, campaign engagement, and where purchases were made.

Here is the link to our data set on Kaggle: <https://www.kaggle.com/imakash3011/customer-personality-analysis>

### Description of Each Attribute

**People**

-   ID: Customer's unique identifier

-   Year_Birth: Customer's birth year

-   Education: Customer's education level

-   Marital_Status: Customer's marital status

-   Income: Customer's yearly household income

-   Kidhome: Number of children in customer's household

-   Teenhome: Number of teenagers in customer's household

-   Dt_Customer: Date of customer's enrollment with the company

-   Recency: Number of days since customer's last purchase

-   Complain: 1 if the customer complained in the last 2 years, 0 otherwise

**Products**

-   MntWines: Amount spent on wine in last 2 years

-   MntFruits: Amount spent on fruits in last 2 years

-   MntMeatProducts: Amount spent on meat in last 2 years

-   MntFishProducts: Amount spent on fish in last 2 years

-   MntSweetProducts: Amount spent on sweets in last 2 years

-   MntGoldProds: Amount spent on gold in last 2 years

**Promotion**

-   NumDealsPurchases: Number of purchases made with a discount

-   AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise

-   AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise

-   AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise

-   AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise

-   AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise

-   Response: 1 if customer accepted the offer in the last campaign, 0 otherwise

**Place**

-   NumWebPurchases: Number of purchases made through the company's website

-   NumCatalogPurchases: Number of purchases made using a catalogue

-   NumStorePurchases: Number of purchases made directly in stores

-   NumWebVisitsMonth: Number of visits to company's website in the last month

## Data Preprocessing & EDA

### Handling Missing Values

First, let's see how much data we are missing:

```{r}
apply(is.na(data), 2, sum)
```

From the output above, we see that the Income column contains 24 missing values. The rest of the columns contain no missing values.

Since 24 missing values is small compared to our total 2240 rows in our dataset, we are going to remove these rows that contain missing values:

```{r}
data <- data %>% filter(complete.cases(data))
head(data)
```

```{r}
count(data)
```

Now, we have 2216 observations in our dataset.

### Now let's explore the variables and perform feature engineering

Here is the total count of each category under "Education":

```{r}
count(data, data["Education"], sort = TRUE)
```

```{r}
ggplot(data, aes(Education, fill = Education)) +
  labs(title = 'Counts for Each Education Level', x = "Education Level") +
  geom_bar()
```

Let's simplify Education by creating only 3 unique values in this column:

```{r}
# convert "2n Cycle" and "Basic" to "Undergraduate"
data["Education"][data["Education"] == "2n Cycle" | 
                    data["Education"] == "Basic"] <- "Undergraduate"
# convert "Graduation" to "Graduate"
data["Education"][data["Education"] == "Graduation"] <- "Graduate"
# convert "Master" and PhD to "Postgrad"
data["Education"][data["Education"] == "Master" | data["Education"] == "PhD"] <- "Postgrad"
# print new counts for Marital_Status column
count(data, data["Education"], sort = TRUE)
```

Let's see how Education relates to Income:

```{r}
# calculate the mean income for each Education Level
plotdata <- data %>%
  group_by(Education) %>%
  summarize(mean_income = mean(Income))

# plot mean incomes
ggplot(plotdata, 
       aes(x = factor(Education,
                      labels = c("Undergraduate", "Graduate", "Postgraduate")),
           y = mean_income)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  geom_text(aes(label = dollar(mean_income)), vjust = -0.25) +
  labs(x = "Education Level", y = "Mean Income")
```

Let's count the number of observations per category in "Martial_Status" as well:

```{r}
count(data, data["Marital_Status"], sort = TRUE)
```

Since "Absurd" and "YOLO" aren't useful to us under this feature, we are going to remove these rows:

```{r}
data <- data[!(data$Marital_Status == "YOLO" | data$Marital_Status == "Absurd"),]
head(data)
```

Like we did with Education, let's create two unique elements in Marital_Status:

```{r}
# convert "Together" to "Married"
data["Marital_Status"][data["Marital_Status"] == "Together"] <- "Married"
# convert "Alone", "Divorced", and "Widow" to "Single"
data["Marital_Status"][data["Marital_Status"] == "Alone" |
                         data["Marital_Status"] == "Divorced" |
                         data["Marital_Status"] == "Widow"] <- "Single"
# print new counts for Marital_Status column
count(data, data["Marital_Status"], sort = TRUE)
```

Assuming we got this data from 2018, let's convert birth years to age:

```{r}
data["Age"] <- 2018 - data["Year_Birth"]
data <- data[,!(names(data) == "Year_Birth")]
```

Let's take a look at the distribution of the ages of the customers:

```{r}
ggplot(data, aes(Age)) +
  labs(title = 'Distribution of Ages of Cusomters') +
  geom_histogram(binwidth = 2)
```

```{r}
sprintf("Youngest Customer: %s", min(data$Age))
sprintf("Oldest Customer: %s", max(data$Age))
```

Our youngest customer is 22 years old and our oldest is 125. Since there seems to be a couple outliers in age in our dataset, let's remove these observations:

```{r}
# remove outliers in age
data <- data[!(data$Age > 100),]
```

Let's continue with our feature engineering and create a column for total spending by combining all of the categories for spending:

```{r}
# Create column for total spending
data["Total_Spending"] <- data["MntWines"] + data["MntFruits"] + data["MntMeatProducts"] + 
                          data["MntFishProducts"] + data["MntSweetProducts"] +  
                          data["MntGoldProds"]
# remove individual columns for amount spent on each product
del_amts <- c("MntWines", "MntFruits", "MntMeatProducts", "MntFishProducts", 
              "MntSweetProducts", "MntGoldProds")
data <- data[,!(names(data) %in% del_amts)]
```

Likewise, let's create a column for the total number of purchases by the customer:

```{r}
data["TotalNumPurchases"] <- data["NumWebPurchases"] + data["NumCatalogPurchases"] +
                            data["NumStorePurchases"] + data["NumDealsPurchases"]
del_NumPurchases <- c("NumWebPurchases", "NumCatalogPurchases", "NumStorePurchases",
                      "NumDealsPurchases")
data <- data[,!(names(data) %in% del_NumPurchases)]
head(data)
```

Let's create a new column for the total number of accepted marketing campaigns by the customer, and a column that indicates whether a customer has accepted a campaign in the past:

```{r}
data["TotalAcceptedCmp"] <- data["AcceptedCmp1"] + data["AcceptedCmp2"] + 
                            data["AcceptedCmp3"] + data["AcceptedCmp4"] + 
                            data["AcceptedCmp5"] + data["Response"]
# create column telling us whether customer responded to any campaign
data$AcceptedCmp <- ifelse(data$TotalAcceptedCmp > 0, 1, 0)
```

```{r}
table(data$AcceptedCmp)
```

Now, let's explore these new variables:

```{r}
# prints distribution of customers who have accpeted a campaign
ggplot(data, aes(x = AcceptedCmp)) + geom_bar(alpha = 0.8)
```

Let's replace the "KidHome" and "Teenhome" columns with a column representing the total number of children at home:

```{r}
# create column for total children at home
data["Children"] <- data["Kidhome"] + data["Teenhome"]
data <- data[,!(names(data) == "Kidhome" | names(data) == "Teenhome")]
```

Let's inspect the remaining columns:

```{r}
unique(data["Z_CostContact"])
```

```{r}
unique(data["Z_Revenue"])
```

Because "Z_CostContact" and "Z_Revenue" only has one unique value, let's delete it:

```{r}
data <- data[,!(names(data) == "Z_CostContact" | names(data) == "Z_Revenue")]
head(data)
```

Delete other unimportant columns:

```{r}
# delete ID and Dt_Customer
del_unimp <- c("ID", "Dt_Customer")
data <- data[,!(names(data) %in% del_unimp)]
# delete complain column
data <- data[,!(names(data) == "Complain")]
```

Now, let's inspect our processed dataset:

```{r}
head(data)
```

Our dataset now consists of .....

### EDA After Feature Engineering

```{r}
plot(Total_Spending~Income, data)
```

There seems to be one clear outlier who has far greater income than the rest of the dataset. Let's remove this observation.

```{r}
data <- data[!(data$Income == max(data$Income)),]
```

```{r}
plot(Total_Spending~Income, data)
```

```{r}
#value will be 1 if a customer has accepted, 0 if customer has not accepted a campaign
data$AcceptedCmp <- as.factor(ifelse(data$TotalAcceptedCmp > 0, 1, 0))
head(data)
```

```{r}
# prints out correlations with AcceptedCmp variable
corr_var(data, AcceptedCmp)
```

```{r}
# calculate the mean total number of purchases amoungst customers
plot_mean <- data %>%
  group_by(AcceptedCmp) %>%
  summarize(mean_num_purchases = round(mean(TotalNumPurchases), digits = 3))

# plot mean total number of purchases
ggplot(plot_mean, 
       aes(x = factor(AcceptedCmp,
                      labels = c("Did not accept", "Accepted")),
           y = mean_num_purchases)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  geom_text(aes(label = mean_num_purchases, vjust = -0.25)) +
  labs(x = "Customer Response", y = "Mean Total Number of Purchases") 
```

```{r}
# calculate the mean total spending amoungst customers
plot_mean <- data %>%
  group_by(AcceptedCmp) %>%
  summarize(mean_total_spending = round(mean(Total_Spending), digits = 3))

# plot mean total number of purchases
ggplot(plot_mean, 
       aes(x = factor(AcceptedCmp,
                      labels = c("Did not accept", "Accepted")),
           y = mean_total_spending)) +
  geom_bar(stat = "identity", fill = "cornflowerblue") +
  geom_text(aes(label = mean_total_spending, vjust = -0.25)) +
  labs(x = "Customer Response", y = "Mean Total Spending") 
```

### Dimension Reduction for Clustering

First, let's label encode the categorical variables:

```{r}
# convert "Undergraduate" to 0
data["Education"][data["Education"] == "Undergraduate"] <- 0
# convert "Graduate" to 1
data["Education"][data["Education"] == "Graduate"] <- 1
# convert "Postgrad" to 2
data["Education"][data["Education"] == "Postgrad"] <- 2
# convert string values to numeric values
data$Education <- strtoi(data$Education)
# print table of Education values
table(data$Education)
```

```{r}
data$Marital_Status <- ifelse(data$Marital_Status == "Married", 1, 0)
table(data$Marital_Status)
```

### PCA

Next, let's perform dimension reduction using PCA:

```{r}
pca = prcomp(data, center = TRUE, scale = TRUE)
summary(pca)
```

```{r}
# create dataframe with only 3 most significant components
pca_3 = as.data.frame(pca$x[,1:3])
```

Determine the optimal number of clusters using elbow method:

```{r}
fviz_nbclust(pca_3, kmeans, method = 'wss')
```

It seems that there is no significant change in total within sum of square at k = 4, so we are going to use 4 clusters.

```{r}
# set the number of clusters
k = 4
# plot the cluster plot with PC1, PC2, and PC3
kmeans_clus = kmeans(pca_3, centers = k, nstart = 50)
fviz_cluster(kmeans_clus, data = pca_3)
```

## Evaluating the Clusters

First, let's assign each row in the data to their respective clusters:

```{r}
data$cluster <- as.factor(kmeans_clus$cluster)
head(data)
```

Let's look at the distribution of clusters:

```{r}
# set colors for consistency
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
n = 4
cols = gg_color_hue(n)

# distribution of clusters
ggplot(data, aes(x = cluster)) + geom_bar(alpha = 0.8, fill = cols)
```

Now, let's look at some univariate plots to give us information about the clusters we formed:

```{r}
ggplot(data, 
        aes(x = Total_Spending, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Total spending distribution by cluster",
        x = "Total Spending")
```

```{r}
ggplot(data, 
       aes(x = cluster, 
           y = Total_Spending)) +
  geom_boxplot(fill = cols, alpha = 0.5) +
  labs(title = "Salary distribution by rank",
       x = "Clusters",
       y = "Total Spending")
```

From the visuals above, it looks like the clusters differ greatly in total spending. Cluster 1 and 4 consists of customers who spent a small amount compared to cluster 2 and 3.

```{r}
ggplot(data, 
        aes(x = TotalNumPurchases, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Total # of Purchases by cluster")
```

```{r}
ggplot(data, 
        aes(x = Income, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Income distribution by cluster")
```

The clusters seem to also differ significantly based on income.

```{r}
ggplot(data, 
        aes(x = NumWebVisitsMonth, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Web visits by cluster",
        x = "Number of Website Visits per Month")
```

It looks like cluster 3 consists of customers who didn't visit the website a lot while cluster 4 contains customers who frequently visited the website.

```{r}
ggplot(data, 
        aes(x = Recency, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Recency by cluster",
        x = "# of days since customers last purchase")
```

The number of days since the customers last purchase doesn't seem to be a significant differentiating factor between clusters.

```{r}
ggplot(data, 
        aes(x = Age, 
            fill = cluster)) +
   geom_density(alpha = 0.4) +
   labs(title = "Age distribution by cluster")
```

Cluster 4 seems to consist of a younger population of customers.

```{r}
# calculate the mean age for each cluster
mean_age <- data %>%
  group_by(cluster) %>%
  summarize(mean(Age))
mean_age
```

Let's take a look at the education levels of each cluster:

```{r}
ggplot(data,
       aes(x = cluster, 
           fill = as.character(Education))) + 
  geom_bar(position = position_dodge(preserve = "single")) + 
  scale_fill_discrete(name = "Education Level",
                      label = c('Undergraduate', 'Graduate', 'Postgrad'))
```

It looks like cluster 1 consists of customers were are highly educated while cluster 4 consists of customers who have a relatively low level of education.

```{r}
ggplot(data,
       aes(x = cluster, 
           fill = as.character(Marital_Status))) + 
  geom_bar(position = position_dodge(preserve = "single")) + 
  scale_fill_discrete(name = "Marital Status",
                      label = c('Single', 'Married'))
```

It doesn't look like there is much differentiation in marital status between the clusters. However, cluster 3 seems to contain a lesser proportion of customers who are married.

Let's see if number of children is significant:

```{r}
ggplot(data,
       aes(x = cluster, 
           fill = as.character(Children))) + 
  geom_bar(position = position_dodge(preserve = "single")) + 
  scale_fill_discrete(name = "# of Children")
```

It looks like cluster 1 contains customers who have more children while cluster 3 contains customers who don't have many children.

Now, let's inspect the total number of accepted promotions per cluster:

```{r}
ggplot(data,
       aes(x = cluster, 
           fill = as.character(TotalAcceptedCmp))) + 
  geom_bar(position = position_dodge(preserve = "single")) + 
  scale_fill_discrete(name = "Total Accepted Promos")
```

Although the number of accepted promotions was underwhelming overall, it seems that cluster 3 contains most of the customers who accepted promotions.

```{r}
p <- ggplot(data, aes(x = Income,
                      y = Total_Spending, colour = cluster)) +
  geom_point(size = 1,
             alpha = .7) + 
  labs(x = "Income",
       y = "Total Spending",
       title = "Income vs Total Spending")

ggMarginal(p, data, x = Income, y = Total_Spending,
           type = "density", groupColour = TRUE, groupFill = TRUE)
```

# Data Split

The data was split into a 80/20 training and test split.

```{r}
data_split <- data %>% 
  initial_split(prop = 0.8)
data_train <- training(data_split)
data_test <- testing(data_split)
```

## Exploratory Data Analysis

First, lets explore our data through EDA. Let's better understand our customers... can we segment our customers based on identifiable characteristics...?

### K-Means Clustering

For the sake of reducing redundancy and the dimensions of our dataset as much as possible, let's combine the customer responses to all of the campaigns.

```{r}
data["TotalAcceptedCmp"] <- data["AcceptedCmp1"] + data["AcceptedCmp2"] + 
                            data["AcceptedCmp3"] + data["AcceptedCmp4"] + 
                            data["AcceptedCmp5"] + data["Response"]
del_accep_cmp <- c("AcceptedCmp1", "AcceptedCmp2", "AcceptedCmp3", "AcceptedCmp4", 
                   "AcceptedCmp5", "Response")
data <- data[,!(names(data) %in% del_accep_cmp)]
head(data)
```

## Predicting Customer Response to Promotions

Now that we have a good idea of our customer base and that we have segmented them into 4 groups, lets see which customers respond well to promotions.

We are going to predict a customer's response to a marketing campaign using 3 different classification models: Logistic Regression, Boosting, and Random Forest.

### Logistic Regression

First, let's look at the logistic regression model:

JOSH

### Boosting

```{r}
set.seed(1)
boost.data = gbm(AcceptedCmp~., data=data_train, distribution="bernoulli", n.trees=500, interaction.depth=4)
summary(boost.data)
```

Total_Spending and income are the most influential predictors. We can also create partial dependence plots for these variables which would illustrate the marginal effect of the selected variables on the response after integrating out the other variables.

```{r}
par(mfrow = c(1,2))
plot(boost.data, i='Total_Spending')
```

```{r}
plot(boost.data, i= 'Income')
```

And lastly, we can use the boosted model to predict on the test set.

```{r}
yhat.boost = predict(boost.data, newdata = data_test, n.trees = 500, type = "response")
yhat.boost = ifelse(yhat.boost > 0.5, 1, 0)
test.boost.error = mean(yhat.boost != ifelse(data_test$AcceptedCmp == "1", 1, 0))
test.boost.error
```

Ans: The test error for boosting is 0.167.

### Random Forest

#### Create Dummy variable

Create binary dummy variables (0 and 1) for three categorical variables, Education, Marital_Status, and AcceptedCmp. Then remove the original columns.

```{r}
library( fastDummies )
df1 <- dummy_cols(data,
                  select_columns = c("Education", "Marital_Status","AcceptedCmp"),
                           remove_first_dummy = TRUE,
                           remove_selected_columns = TRUE )
```

#### Splitting Data into training and test sets

We will take our dataset and divided it into two subsets: the train and test subsets. The training set will be used to fit the machine learning model while the testing/validation set will be used to evaluate the fit of the machine learning model.

In this random forest model, we split our data in to 70% train and 30% validation sets.

```{r}
df1_subset <- df1[ , -c(5:9)]

set.seed(123)
train.rows <- sample(rownames( df1_subset), nrow( df1_subset )*0.7)
train.data <- df1_subset[train.rows , ]
test.rows <- setdiff(rownames( df1_subset), train.rows)
test.data <- df1_subset[test.rows , ]
```

#### Building the random forest model

```{r}
library(randomForest)
set.seed(123)
df1_rf_promo <- randomForest(as.factor(TotalAcceptedCmp)  ~ ., data=train.data, ntree=500,mtry = 4)

df1_rf_promo
```

From the training data to fit the random forest model, we see that by calling "df1_rf_promo" that the the Out of Bag (OOB) estimate of error rate is 10.68% for 500 trees. The OOB error is the mean prediction error on each of the training samples.

The confusion matrix is a good way of assessing the performance of the classifier when it is presented with new data.

```{r}
plot(df1_rf_promo)
```

From the plot, out of 500 trees, we see that improvement steadies out as the number of trees increase. We see from the plot that the error is the lowest around less than 100 trees or at 100 trees.

```{r}
importance(df1_rf_promo)
```

```{r}
varImpPlot(df1_rf_promo, sort=T, main="Variable Importance for df1_rf_promo")
```

From the importance() function and varImpPlot(), we are able to both quantify and visualize the importance of each variable. From both the table and plot, we see that the most significant variables in determining a customer's response to a promotion are "AcceptedCmp_1", "NumDaysEnrolled", "Income", "Recency", and "total_spent".

From this information, this indicates that the most significant factors in determining a customers' response to a promotion are if that customer has responded to or accepted promotions before, the number of days they've enrolled with the company, their income, the number of days since their last purchase, and the amount they spend.

```{r}
varImpPlot(df1_rf_promo, sort=T, main="Top 5 Variables' Importance for df1_rf_promo", n.var=5)
```

Here, we can visualize with better ease of the top five variables' importance in determining a customer's response to a promotion or campaign.

### Cross-Validation

We will perform cross-validation by examining the validation error rate and fine tuning one of the parameters for the random forest model (mtry).

First, we will find the test set error rate.

```{r}
set.seed(123)
yhat.rf = predict (df1_rf_promo, newdata = test.data)
# Confusion matrix
rf.err = table(pred = yhat.rf, truth = test.data$TotalAcceptedCmp)
test.rf.err = 1 - sum(diag(rf.err))/sum(rf.err)
test.rf.err
```

The test set error rate is 0.09803922. This indicates that this model's accuracy is a great improvement.

#### Fine Tuning by finding the best mtry

Two parameters that are important in the random forest algorithm are the number of trees used in the forest, and the number of random variables used in each tree.

Here, we see if we can find the best mtry for the random forest model.

One of the biggest factors that has the biggest effect on the final accuracy of the model is the mtry. The mtry is the number of variables randomly sampled as candidates at each split. As we see from the figure below, as the number of predictors sampled increases, the error goes down. However, having a high mtry can also make the model prone to overfitting.

```{r}
mtry <- tuneRF(train.data[-20], train.data$TotalAcceptedCmp, ntreeTry=500, 
               stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE)
```

**The total amount spent on products** are very highly correlated with whether the customer responded to the marketing campaign.

In the end, we find that **Income and Total Amount Spent are very correlated**. Customers who earn more spend more.

**Customers who have recently purchased something aremore likely to respond** to the marketing campaign. This makes senses because when a customer has more recent purchases they have more of a probable pattern of shopping at the store.

### Discussion of our best-fitting model

Compare logistic regression vs random forest

## Conclusion

Everyone, after all work is done
