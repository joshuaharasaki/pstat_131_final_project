---
title: "katiework2"
output: html_document
date: '2022-03-12'
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)

# install.packages("randomForest")
library(randomForest)
#install.packages("gbm")
library(gbm)
library(ISLR)
library(tree)
```

## Loading Data

```{r}
data = read.csv('marketing_campaign.csv', sep = '\t')  # load data
head(data)  # print first 5 rows of data
```

```{r}
df = data.frame(data)
```

#### Check for and remove missing values

```{r}
missing.values <- df %>%
  gather(key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  group_by(key) %>%
  mutate(total = n()) %>%
  group_by(key, total, isna) %>%
  summarise(num.isna = n()) %>%
  mutate(pct = num.isna / total * 100)
```

```{r}
df1=na.omit(df)
```

#### Calculating, combining, and creating variables

Combining the variables to get the number of children from the customers

```{r}
df1['Age']= 2022 - df1$Year_Birth
df1['Child']=df1$Kidhome+df1$Teenhome
```

```{r}
print(min(df1$Dt_Customer))
print(max(df1$Dt_Customer))

```

Combining the spend amounts for items

```{r}
df1['total_spent']=df1$MntMeatProducts+df1$MntFishProducts+df1$MntWines+df1$MntFruits+df1$MntSweetProducts+df1$MntGoldProds
```

Combining the number of children

```{r}
df1["Children"] <- df1["Kidhome"] + df1["Teenhome"]
df1 <- df1[,!(names(data) == "Kidhome" | names(df1) == "Teenhome")]
head(df1)
```

Create a column for the total number of accepted promotions

```{r}
df1["TotalAcceptedCmp"] <- data["AcceptedCmp1"] + data["AcceptedCmp2"] +
  data["AcceptedCmp3"] + data["AcceptedCmp4"] + data["AcceptedCmp5"] + data["Response"]

del_accep_cmp <- c("AcceptedCmp1", "AcceptedCmp2", "AcceptedCmp3", "AcceptedCmp4", "AcceptedCmp5", "Response")

df1 <- df1[,!(names(df1) %in% del_accep_cmp)]
head(df1)
```

Number of day customer enroll with the company (until 07/31/2021)

-   Calculate days difference between the day customer enrolled with the company and the day the author uploaded the data on Kaggle, 07/31/2021.

```{r}
df$Dt_CustomerCovert1 = as.Date(df$Dt_Customer)
df$Dt_CustomerCovert2 = as.Date("2021-07-31") - as.Date(df$Dt_CustomerCovert1)
df$NumberofDayEnrolled = as.numeric(df$Dt_CustomerCovert2, units="days")
```

\

```{r}
head(df1)
```

```{r}
names(df1)
```

#### Remove unnecessary variables

We only need the customers' general information to predict the amount spent on different product categories.

```{r}
# 1 - ID : 2 - Year_Birth
# 8 - Dt_Customer : 9 - Recency
# 16 - NumDealsPurchases : 25 - AcceptedCmp2  
# 27 - Z_CostContact : 29 - Response           
# 31 - Dt_CustomerCovert1 : 32 - Dt_CustomerCovert2 
df1 <- df[-c( 1:2 , 8:9 , 28:29 , 31:33 )]
```

```{r}
head(df1)
```

#### Detecting and Removing Outliers

```{r}
df1 <- df1[!(df1$Income>150000 | 
            df1$MntMeatProducts>1000 |
            df1$MntSweetProducts>200 | 
            df1$MntGoldProds>260 ) , ]
```

#### Handle missing data

```{r}
df1 <- df1[!(is.na(df1$Income)),]
```

#### Convert the categories "Education", "Marital_Status", and "Complain" to factors

```{r}
df1$Education <- as.factor(df1$Education)
df1$Marital_Status <- as.factor(df1$Marital_Status)
df1$Complain <- as.factor(df1$Complain)
df1$AcceptedCmp <- as.factor(ifelse(df1$TotalAcceptedCmp > 0, 1, 0))
```

## Reducing Categories

#### See how frequently each marital status occurs

```{r}
MarritalStatfreq <- data.frame(table(df1$Marital_Status))
MarritalStatfreq[order(MarritalStatfreq$Freq, decreasing = TRUE),]
```

#### List marital status that appear in the at least 1% of the records

```{r}
MarritalStatfreq[MarritalStatfreq$Freq / nrow(df1) > .01, ]
```

#### Combine all other marital status into "Other"

8 statuses for "Marital Status" variable

Only 5 appear in at least 1% of the records, so other 3 will go into "Other"

```{r}
df1$Marital_Status <- as.factor(ifelse(df1$Marital_Status %in% 
                                               c("Divorced", "Married", "Single","Together","Widow"), 
                                        as.character(df1$Marital_Status), 
                                        "Other"))
MarritalStatfreq <- data.frame(table(df1$Marital_Status))
MarritalStatfreq[order(MarritalStatfreq$Freq, decreasing = TRUE),]
```

#### Create Dummy variable

Create binary dummy variables (0 and 1) for three categorical variables, Education, Marital_Status, and Complain. Then remove the original columns.

```{r}
library( fastDummies )
df1 <- dummy_cols( df1,                                              
                           select_columns = c("Education", "Marital_Status","Complain", "AcceptedCmp" ),
                           remove_first_dummy = TRUE,
                           remove_selected_columns = TRUE )
```

#### Splitting Data into training and test sets

Splitting into 70% train, 30% test data

```{r}
df1_subset <- df1[ , -c(5:9)]

set.seed(123)
train.rows <- sample(rownames( df1_subset), nrow( df1_subset )*0.7)
train.data <- df1_subset[train.rows , ]
test.rows <- setdiff(rownames( df1_subset), train.rows)
test.data <- df1_subset[test.rows , ]
```

```{r}
head(train.data)
```

``` {head(train.rows)}
```

```{r}
#checking the dimensions
#dim(test)       
#dim(train)
```

### Random Forest

combination predictions from many trees

#### Building the random forest model

```{r}
library(randomForest)
df1_rf <- randomForest(MntWines ~ . , 
                            data = train.data, 
                            ntree = 500,
                            mtry = 4, 
                            nodesize = 5, 
                            importance = TRUE)
df1_rf
```

We see from calling "df1_rf" that the variance explained is relatively high at around 60.35% with 500 trees created.

#### Plotting the random forest model

```{r}
plot(df1_rf)
```

We plot the number of trees by the mean squared error. As you can see, as there are more trees there is less error to a certain point. It looks as though around 300-400 trees is enough. To confirm this guess, we use the "which.min" function.

We determine how many trees are optimal by looking at a plot and then using the "which.min" function.

```{r}
which.min(df1_rf$mse)
```

Here, it is confirmed that we need 317 trees to have the lowest error. Thus, we will rerun the model with 317 trees.

```{r}
# Remaking the random forest with 317 trees
df317_rf <- randomForest(MntWines ~ . , 
                            data = train.data, 
                            ntree = 317,
                            mtry = 4, 
                            nodesize = 5, 
                            importance = TRUE)
df317_rf
```

```{r}
plot(df317_rf)
```

The model is still not great, with the variance explained and the error decreased slightly.

#### View the importance of each variable

```{r}
importance(df1_rf)
```

```{r}
varImpPlot(df1_rf)
```

Income is the most important predictor for customer spending, followed by the number of kids.

```{r}
varImpPlot(df1_rf, sort=T, main="Variable Importance for df1_rf", n.var=5)
```

Cross-Validation

#### Finding the test set error rate

```{r}
yhat.rf = predict (df1_rf, newdata = test.data)
# Confusion matrix
rf.err = table(pred = yhat.rf, truth = test.data$MntWines)
test.rf.err = 1 - sum(diag(rf.err))/sum(rf.err)
test.rf.err
```

The test set error rate is 0.9954.

Confusion Matrix

```{r}
z = sample(n,200)
yhat.rf = predict (df1_rf, newdata = test.data)
#mean((yhat.rf - ))
#Confusion matrix
#confusionMatrix(df1_rf.pred, as.factor(test.data$Income), positive = "1")
#cm = table(test.data, yhat.rf)
#print(cm)
```

#### RMSE

```{r}
df1_rf.pred <- predict(df1_rf, test.data)
RMSE(df1_rf.pred, test.data$MntWines)
```

The mean-square error of prediction, estimated by the validation set cross-validation is 214.1775.

```{r}
mtry <- tuneRF(train.data[-20], train.data$Income, ntreeTry=500, 

               improve=0.01, trace=TRUE, plot=TRUE)
```

Plot the predicted values (out-of-bag estimation) vs. true values if treating the training data as new values

```{r}
plot( predict(df1_rf,newdata=train.data), )
```

#### Model with the Important variables

```{r}
df1_rf_imp <- randomForest(MntWines ~ Income+Kidhome+Education_PhD+Teenhome, data=train.data, ntree=500,mtry =4)
plot(df1_rf_imp)
```

## Predicting whether a customer will respond to a promotion or not

```{r}
df1_rf_promo <- randomForest(TotalAcceptedCmp  ~ ., data=train.data, ntree=500,mtry =4)
df1_rf_promo
```

```{r}
importance(df1_rf_promo)
```

```{r}
varImpPlot(df1_rf_promo, sort=T, main="Variable Importance for df1_rf_promo")
```

```{r}

```
